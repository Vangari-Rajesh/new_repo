{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34fa6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96cc58c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define your transformation\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15004f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "# Path for the main dataset directory\n",
    "dataset_path = r\"E:\\g42\\cattle_final_dataset\"\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = torchvision.datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create the custom datasets\n",
    "train_dataset = CustomDataset(os.path.join(dataset_path, 'train'), transform=transformer)\n",
    "test_dataset = CustomDataset(os.path.join(dataset_path, 'test'), transform=transformer)\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70342e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['054_muzzle', '150_muzzle', '151_muzzle', '152_muzzle', '153_muzzle', '154_muzzle', '155_muzzle', '156_muzzle', '157_muzzle', '158_muzzle', '159_muzzle', '160_muzzle', '161_muzzle', '162_muzzle', '163_muzzle', '164_muzzle', '165_muzzle', '166_muzzle', '167_muzzle', '168_muzzle', '169_muzzle', '170_muzzle', '171_muzzle', '172_muzzle', '173_muzzle', '174_muzzle', '175_muzzle', '176_muzzle', '177_muzzle', '178_muzzle', '179_muzzle', '180_muzzle', '181_muzzle', '182_muzzle', '183_muzzle', '184_muzzle', '185_muzzle', '186_muzzle', '187_muzzle', '188_muzzle', '189_muzzle', '190_muzzle', '191_muzzle', '192_muzzle', '193_muzzle', '194_muzzle', '195_muzzle', '196_muzzle', '197_muzzle', '198_muzzle', '199_muzzle', '200_muzzle', '201_muzzle', '204_muzzle', '205_muzzle', '206_muzzle', '207_muzzle', '208_muzzle', '209_muzzle', '210_muzzle', '211_muzzle', '212_muzzle', '213_muzzle', '214_muzzle', '215_muzzle', '216_muzzle', '217_muzzle', '218_muzzle', '220_muzzle', '221_muzzle', '222_muzzle', '224_muzzle', '225_muzzle', '226_muzzle', '227_muzzle', '228_muzzle', '229_muzzle', '230_muzzle', '231_muzzle', '232_muzzle', '233_muzzle', '234_muzzle', '235_muzzle', '236_muzzle', '237_muzzle', '238_muzzle', '239_muzzle', '240_muzzle', '241_muzzle', '242_muzzle', '243_muzzle', '244_muzzle', '245_muzzle', '246_muzzle', '247_muzzle', '248_muzzle', '249_muzzle', '250_muzzle', '300_muzzle', '302_muzzle', '303_muzzle', '304_muzzle', '305_muzzle', '306_muzzle', '307_muzzle', '308_muzzle', '309_muzzle', '310_muzzle', '312_muzzle', '313_muzzle', '314_muzzle', '315_muzzle', '316_muzzle', '317_muzzle', '319_muzzle', '321_muzzle', '323_muzzle', '324_muzzle', '325_muzzle', '326_muzzle', '327_muzzle', '328_muzzle', '329_muzzle', '330_muzzle', '331_muzzle', '332_muzzle', '333_muzzle', '334_muzzle', '335_muzzle', '336_muzzle', '337_muzzle', '338_muzzle', '339_muzzle', '340_muzzle', '341_muzzle', '342_muzzle', '343_muzzle', '344_muzzle', '345_muzzle', '346_muzzle', '347_muzzle', '348_muzzle', '349_muzzle', '350_muzzle', '351_muzzle', '352_muzzle', '353_muzzle', '354_muzzle']\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "#categories\n",
    "root=pathlib.Path(os.path.join(dataset_path, 'train'))\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "\n",
    "print(classes)\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "805acfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#checking for device\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c6a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train the model incrementally i.e fine-tuning\n",
    "\n",
    "\n",
    "# Define your transformation\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Path to the existing model\n",
    "model_path = 'cattle_aadhar_copy.model'\n",
    "\n",
    "# Path for the new data\n",
    "new_data_path = r\"E:\\g42\\new_added\"\n",
    "\n",
    "# Define your model architecture\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc = nn.Linear(in_features=75 * 75 * 32, out_features=num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.pool(output)\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = output.view(-1, 32 * 75 * 75)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Load the existing model\n",
    "model = ConvNet(num_classes=len(classes)).to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Update the model's architecture for the combined dataset (146 old IDs + 1 new ID)\n",
    "combined_num_classes = len(classes) +1  # 146 old cattle IDs + 1 new cattle ID (250)\n",
    "model.fc = nn.Linear(in_features=75 * 75 * 32, out_features=combined_num_classes)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Combine the old and new datasets using a custom dataset class\n",
    "# class CombinedDataset(Dataset):\n",
    "#     def __init__(self, dataset1, dataset2):\n",
    "#         self.dataset1 = dataset1\n",
    "#         self.dataset2 = dataset2\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset1) + len(self.dataset2)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if idx < len(self.dataset1):\n",
    "#             return self.dataset1[idx]\n",
    "#         else:\n",
    "#             return self.dataset2[idx - len(self.dataset1)]\n",
    "        \n",
    "# new_data = CustomDataset(os.path.join(dataset_path, new_data_path), transform=transformer)\n",
    "\n",
    "# # Create the combined dataset\n",
    "# combined_dataset = CombinedDataset(train_dataset, new_data)\n",
    "# combined_loader = DataLoader(combined_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# # Fine-tune the model on the combined dataset\n",
    "# model.train()\n",
    "\n",
    "# for epoch in range(10):  # Adjust the number of fine-tuning epochs\n",
    "#     train_accuracy = 0.0\n",
    "#     train_loss = 0.0\n",
    "\n",
    "#     for i, (images, labels) in enumerate(combined_loader):\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(images)\n",
    "#         loss = loss_function(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item() * images.size(0)\n",
    "#         _, prediction = torch.max(outputs.data, 1)\n",
    "#         train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "\n",
    "#     train_accuracy = train_accuracy / len(combined_dataset)\n",
    "#     train_loss = train_loss / len(combined_dataset)\n",
    "\n",
    "#     print(f'Fine-Tuning Epoch: {epoch}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}')\n",
    "\n",
    "# # Save the updated model\n",
    "# torch.save(model.state_dict(), 'cattle_aadhar_updated.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1217af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # after fine-tuning to remove the old model which is named as cattle_aadhar_copy.model and rename the new model cattle_aadhar_updated.model as cattle_aadhar_copy.model\n",
    "\n",
    "\n",
    "# import os\n",
    "\n",
    "# # Delete the old model\n",
    "# old_model_path = 'cattle_aadhar_copy.model'\n",
    "# if os.path.exists(old_model_path):\n",
    "#     os.remove(old_model_path)\n",
    "#     print(f\"Deleted old model: {old_model_path}\")\n",
    "\n",
    "# # Rename the updated model\n",
    "# updated_model_path = 'cattle_aadhar_updated.model'\n",
    "# new_model_path = 'cattle_aadhar_copy.model'\n",
    "# os.rename(updated_model_path, new_model_path)\n",
    "# print(f\"Renamed updated model to: {new_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84632ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to check what folders are present in new_added folder\n",
    "\n",
    "# import os\n",
    "\n",
    "# directory_path = r\"E:\\g42\\new_added\"  # Replace with the path to your directory\n",
    "\n",
    "# # Get a list of all items (files and folders) in the directory\n",
    "# items = os.listdir(directory_path)\n",
    "\n",
    "# # Filter the items to get only the folders (directories)\n",
    "# folders = [item for item in items if os.path.isdir(os.path.join(directory_path, item))]\n",
    "\n",
    "# # Print the list of folders\n",
    "# for folder in folders:\n",
    "#     f=folder\n",
    "# print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to move the newly trained cattle to already trained cattles\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# source_path = r\"E:\\g42\\new_added\"\n",
    "\n",
    "# source_folder = os.path.join(source_path,f)  # Replace with the path to the source folder\n",
    "# destination_folder = r\"E:\\g42\\cattle_final_dataset\\train\"  # Replace with the path to the destination folder\n",
    "\n",
    "# # Move the '250_muzzle' folder to the destination folder\n",
    "# shutil.move(source_folder, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d8fb312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc): Linear(in_features=180000, out_features=148, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create an instance of your model\n",
    "model = ConvNet(num_classes=len(classes))  # Replace with your actual model class and configuration\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load('cattle_aadhar_copy.model'))\n",
    "\n",
    "# Set the model in evaluation mode (important if you're using dropout or batch normalization)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d4bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba6c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming tensor1 and tensor2 are your two tensors\n",
    "tensor1 = torch.tensor([[  6.5371, -19.3360, -19.0776, -14.4396, -16.8002, -22.5280,  -7.6527,\n",
    "         -17.9633, -16.5074, -14.0633, -18.8894, -25.7352, -25.2688, -12.9061,\n",
    "         -13.2991, -18.6366, -15.0103, -24.0876, -21.3251, -22.3753, -18.7472,\n",
    "         -20.5405, -16.4260, -16.0588, -24.2321,  -7.4647, -17.2443, -18.1174,\n",
    "          -6.5236, -13.5583,  -4.8678, -14.8383, -12.2263, -15.8481, -22.3048,\n",
    "         -11.0926, -18.2959,  -6.2079, -23.2852, -16.9684, -30.2456, -21.4383,\n",
    "         -20.7051, -14.9434, -20.9345, -20.2689, -24.2631,  -2.7388, -24.5607,\n",
    "          -5.7049, -10.7160, -10.1493, -13.5380, -13.0908, -15.1456, -11.6808,\n",
    "          -8.5991, -18.8080, -21.1398, -20.2393, -10.5309, -18.8499, -21.3764,\n",
    "         -19.8163,  -5.9118, -27.1839, -10.5513, -14.9402, -21.9184, -20.0329,\n",
    "         -22.2188, -19.6090, -22.1819, -13.2962, -21.6181, -10.5572, -12.7969,\n",
    "         -17.0915,  -9.2038, -17.3579, -14.4668, -17.7998, -15.8298, -20.7652,\n",
    "          -5.7256,  -4.8463, -14.7671, -15.4504, -21.5040, -12.9719,  -2.6970,\n",
    "         -19.7600, -19.2650, -13.5190, -21.9013, -15.4935, -18.7133,  -4.1327,\n",
    "         -11.0829, -10.8150, -17.7478, -20.1107, -13.3786, -19.7994, -16.3275,\n",
    "          -7.4085, -13.2405, -14.3043, -12.3779, -19.9927, -22.0098, -14.3770,\n",
    "          -5.3350, -18.6119,  -7.3580, -10.5836, -22.4312, -18.6632, -17.0329,\n",
    "         -10.6835, -25.3300, -27.0039, -16.6436,  -8.7019, -10.4306, -15.7599,\n",
    "         -23.0024, -13.1891, -14.7553, -16.1825, -29.4267, -21.9308, -12.9502,\n",
    "         -10.3692, -12.3828, -18.2302, -16.5014, -22.5469, -20.5992, -19.9890,\n",
    "         -12.0633, -14.3399, -20.1560, -12.3453,  -9.5356,  -0.4668, -44.5037]])\n",
    "\n",
    "tensor2 = torch.tensor([[  9.5424, -23.6340, -29.0779, -24.3913, -25.1843, -31.2242, -15.2095,\n",
    "         -22.2791, -28.1671, -21.9873, -30.5131, -36.7235, -32.1511, -20.7674,\n",
    "         -16.7840, -25.5771, -20.5994, -38.3753, -15.8138, -32.1284, -28.4576,\n",
    "         -16.1448, -27.9621, -21.5243, -28.2380,  -4.5522, -20.0330, -26.3655,\n",
    "         -15.0433, -18.7214, -15.4056, -16.2614, -13.6040, -21.8995, -27.2476,\n",
    "         -12.6512, -26.8652,  -8.4423, -29.6405, -23.4859, -33.6291, -28.0008,\n",
    "         -27.8274, -28.4335, -23.8277, -18.0989, -27.3182, -16.7000, -29.8366,\n",
    "         -15.2044, -16.0721, -18.3276, -32.3414, -20.0376, -27.0423, -22.2444,\n",
    "         -24.4003, -16.8287, -26.7648, -33.3757,  -4.9305, -15.7985, -15.6606,\n",
    "         -25.4034, -16.2415, -38.1160, -11.5713, -15.6831, -34.3122, -31.2038,\n",
    "         -29.0544, -22.1235, -33.5313, -18.9547, -33.8798, -20.0551, -21.3525,\n",
    "         -22.6087, -11.3375, -27.7518, -28.0023, -21.3714, -21.5219, -32.4895,\n",
    "          -9.5645,  -5.6787, -19.5105, -22.5781, -24.9135, -32.8573, -10.2477,\n",
    "         -23.8981, -24.1108, -12.5385, -16.2015, -15.7753, -23.5596,  -5.4514,\n",
    "         -15.2890, -12.7940, -30.9049, -31.1625, -26.3564, -35.4481, -22.6666,\n",
    "          -9.8406, -23.8854, -21.4597, -21.2447, -27.8674, -29.4481, -26.5718,\n",
    "         -16.3144, -24.2873, -17.0973, -21.7754, -29.5936, -25.6418, -24.4927,\n",
    "         -19.6083, -31.7156, -34.0437, -26.5027, -16.6187, -19.2571, -23.9305,\n",
    "         -30.0099, -26.0544, -29.3201, -23.2846, -35.4558, -32.6372, -18.0226,\n",
    "         -28.2430, -28.3306, -22.8457, -22.6480, -31.5130, -31.9289, -30.4340,\n",
    "         -20.6324, -20.8454, -31.4170, -16.9806,  -9.1389,  -5.7061, -62.6420]])\n",
    "\n",
    "# tensor3 = torch.tensor([[  6.6506, -19.5557, -24.4939, -19.8511, -26.3727, -29.8079, -13.8146,\n",
    "#          -19.8867, -28.0225, -16.1575, -26.1401, -35.0539, -31.0443, -15.5158,\n",
    "#          -11.5478, -22.4979, -15.9906, -31.2238, -20.0162, -24.4720, -25.3488,\n",
    "#          -19.7117, -22.6284, -18.2487, -29.1556,  -7.8666, -23.8772, -23.7595,\n",
    "#          -12.6846, -16.9903, -13.1534, -17.7321, -14.4833, -20.1446, -22.1767,\n",
    "#          -12.2012, -25.5303,  -8.1958, -28.7319, -22.1111, -34.1862, -23.2008,\n",
    "#          -21.3176, -24.8225, -20.3983, -20.6235, -25.7248,  -9.7157, -28.2223,\n",
    "#           -9.8268, -15.0869, -16.4228, -25.1706, -18.7951, -20.8164, -20.9963,\n",
    "#          -14.8422, -18.5897, -27.2247, -28.9047, -12.0006, -18.9858, -19.9130,\n",
    "#          -24.3318, -10.0797, -32.3742,  -7.2027, -16.0642, -29.0359, -27.2658,\n",
    "#          -25.1641, -16.0646, -25.2300, -19.1700, -25.6453, -19.9797, -20.8379,\n",
    "#          -16.2871, -11.1865, -24.2838, -22.9473, -24.4453, -19.1783, -26.3323,\n",
    "#           -5.7410,  -7.0940, -17.0841, -21.4742, -22.4471, -22.2434,  -6.8170,\n",
    "#          -21.8467, -23.1927, -13.0653, -18.5982, -17.4860, -24.4700,  -5.7101,\n",
    "#          -10.6467, -15.1693, -22.1948, -24.9848, -17.0153, -31.6902, -20.0562,\n",
    "#          -10.1024, -20.4604, -19.8150, -20.5040, -28.7850, -30.1360, -22.8967,\n",
    "#          -12.0286, -25.2543,  -9.8078, -14.3832, -25.3405, -25.1746, -21.3535,\n",
    "#          -14.4332, -27.2321, -33.0637, -23.6632, -17.6941, -19.1897, -17.2949,\n",
    "#          -28.7850, -18.8521, -23.2632, -19.8440, -33.8522, -26.3803, -14.0426,\n",
    "#          -19.0468, -22.1209, -21.0297, -17.8499, -29.9843, -27.2982, -21.0777,\n",
    "#          -15.8584, -16.4656, -29.2091, -15.5814,  -9.6041,  -3.8786, -57.9118]])\n",
    "\n",
    "# tensor4 = torch.tensor([[ -3.4824, -10.8680,  -8.1529,   5.4700, -11.6167, -17.2581, -10.9423,\n",
    "#          -12.9003, -10.1830, -12.9856,  -9.6973, -28.6968, -24.4891, -10.1501,\n",
    "#          -14.8255, -11.5762,  -8.8104, -11.5376, -25.0054, -14.5749, -16.0652,\n",
    "#          -17.5483, -14.1089, -12.6145, -11.3810, -16.7445, -25.0733, -12.6694,\n",
    "#          -11.9169, -15.8379, -10.8573, -21.8769, -18.8176, -21.1937, -13.4471,\n",
    "#          -19.0147,  -8.0860, -16.4221, -14.8214, -13.9633, -27.5173, -15.7050,\n",
    "#           -7.1690, -17.3499, -16.4446, -15.7085, -15.6009,  -8.1334,  -9.7322,\n",
    "#           -7.3527, -19.2011, -11.4397,  -9.4702, -13.8731, -17.1898,  -9.3663,\n",
    "#          -10.4188, -15.9549, -14.1349, -14.2393, -15.6420,  -9.8525, -17.2397,\n",
    "#          -18.5343,  -6.2280, -18.1943, -12.3533, -12.8807, -18.0223, -23.3994,\n",
    "#          -17.5456,  -8.2631, -15.1875,  -8.4856, -20.1222, -15.2967, -14.8282,\n",
    "#           -7.8134, -12.3526, -17.2257, -17.8997, -10.4578, -17.3465,  -9.4866,\n",
    "#          -18.0180, -19.3010, -19.5958, -14.7932, -12.2732, -12.9189,  -5.4484,\n",
    "#           -6.3284, -12.7052, -13.9353, -15.6860, -22.2755, -14.8091,  -8.7975,\n",
    "#          -10.0313, -22.6271, -20.6745, -13.3297, -11.3387, -18.8216, -15.5083,\n",
    "#           -6.1811,  -7.1955, -20.7856, -14.9561, -14.6680, -16.2311, -16.2188,\n",
    "#          -14.5527, -11.3940, -12.9701,  -8.7783, -11.0011,  -7.1113, -15.3198,\n",
    "#           -8.4124, -12.8897, -11.7705,  -6.7462, -16.0557, -14.1750, -10.6590,\n",
    "#          -21.2112, -12.2705, -13.9947,  -9.9216,  -9.9698, -12.3600,  -9.9378,\n",
    "#          -14.3154, -13.8376,  -8.2310,  -9.6981, -18.8056, -30.9564, -18.6544,\n",
    "#           -5.7622, -10.9579, -11.6333,  -7.4965,  -9.7484, -13.2656, -46.1898]])\n",
    "\n",
    "# # # Calculate the Euclidean distance\n",
    "# # distance12 = torch.norm(tensor1 - tensor2)\n",
    "# # distance13 = torch.norm(tensor1 - tensor3)\n",
    "# # distance23 = torch.norm(tensor2 - tensor3)\n",
    "# # distance14 = torch.norm(tensor1 - tensor4)\n",
    "# # distance24 = torch.norm(tensor2 - tensor4)\n",
    "# # distance34 = torch.norm(tensor3 - tensor4)\n",
    "\n",
    "# # print(distance12)\n",
    "# # print(distance13)\n",
    "# # print(distance23)\n",
    "# # print(distance14)\n",
    "# # print(distance24)\n",
    "# # print(distance34)\n",
    "\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# similarity12 = F.cosine_similarity(tensor1, tensor2)\n",
    "# similarity13 = F.cosine_similarity(tensor1, tensor3)\n",
    "# similarity23 = F.cosine_similarity(tensor2, tensor3)\n",
    "# similarity14 = F.cosine_similarity(tensor1, tensor4)\n",
    "# similarity24 = F.cosine_similarity(tensor2, tensor4)\n",
    "# similarity34 = F.cosine_similarity(tensor3, tensor4)\n",
    "\n",
    "\n",
    "# # Print the distance\n",
    "# print(similarity12)\n",
    "# print(similarity12)\n",
    "# print(similarity23)\n",
    "# print(similarity14)\n",
    "# print(similarity24)\n",
    "# print(similarity34)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7f0c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Coefficient: 0.8143159906393567\n"
     ]
    }
   ],
   "source": [
    "# Convert tensors to NumPy arrays\n",
    "tensor1_np = tensor1.numpy()\n",
    "tensor2_np = tensor2.numpy()\n",
    "\n",
    "# Flatten the NumPy arrays into 1D arrays\n",
    "tensor1_flat = tensor1_np.flatten()\n",
    "tensor2_flat = tensor2_np.flatten()\n",
    "\n",
    "# Calculate the correlation coefficient using NumPy\n",
    "correlation = np.corrcoef(tensor1_flat, tensor2_flat)[0, 1]\n",
    "\n",
    "print(f\"Correlation Coefficient: {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3ba73fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['054_muzzle', '150_muzzle', '151_muzzle', '152_muzzle', '153_muzzle', '154_muzzle', '155_muzzle', '156_muzzle', '157_muzzle', '158_muzzle', '159_muzzle', '160_muzzle', '161_muzzle', '162_muzzle', '163_muzzle', '164_muzzle', '165_muzzle', '166_muzzle', '167_muzzle', '168_muzzle', '169_muzzle', '170_muzzle', '171_muzzle', '172_muzzle', '173_muzzle', '174_muzzle', '175_muzzle', '176_muzzle', '177_muzzle', '178_muzzle', '179_muzzle', '180_muzzle', '181_muzzle', '182_muzzle', '183_muzzle', '184_muzzle', '185_muzzle', '186_muzzle', '187_muzzle', '188_muzzle', '189_muzzle', '190_muzzle', '191_muzzle', '192_muzzle', '193_muzzle', '194_muzzle', '195_muzzle', '196_muzzle', '197_muzzle', '198_muzzle', '199_muzzle', '200_muzzle', '201_muzzle', '204_muzzle', '205_muzzle', '206_muzzle', '207_muzzle', '208_muzzle', '209_muzzle', '210_muzzle', '211_muzzle', '212_muzzle', '213_muzzle', '214_muzzle', '215_muzzle', '216_muzzle', '217_muzzle', '218_muzzle', '220_muzzle', '221_muzzle', '222_muzzle', '224_muzzle', '225_muzzle', '226_muzzle', '227_muzzle', '228_muzzle', '229_muzzle', '230_muzzle', '231_muzzle', '232_muzzle', '233_muzzle', '234_muzzle', '235_muzzle', '236_muzzle', '237_muzzle', '238_muzzle', '239_muzzle', '240_muzzle', '241_muzzle', '242_muzzle', '243_muzzle', '244_muzzle', '245_muzzle', '246_muzzle', '247_muzzle', '248_muzzle', '249_muzzle', '250_muzzle', '300_muzzle', '302_muzzle', '303_muzzle', '304_muzzle', '305_muzzle', '306_muzzle', '307_muzzle', '308_muzzle', '309_muzzle', '310_muzzle', '312_muzzle', '313_muzzle', '314_muzzle', '315_muzzle', '316_muzzle', '317_muzzle', '319_muzzle', '321_muzzle', '323_muzzle', '324_muzzle', '325_muzzle', '326_muzzle', '327_muzzle', '328_muzzle', '329_muzzle', '330_muzzle', '331_muzzle', '332_muzzle', '333_muzzle', '334_muzzle', '335_muzzle', '336_muzzle', '337_muzzle', '338_muzzle', '339_muzzle', '340_muzzle', '341_muzzle', '342_muzzle', '343_muzzle', '344_muzzle', '345_muzzle', '346_muzzle', '347_muzzle', '348_muzzle', '349_muzzle', '350_muzzle', '351_muzzle', '352_muzzle', '353_muzzle', '354_muzzle']\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "#categories\n",
    "root=pathlib.Path(os.path.join(dataset_path, 'train'))\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "\n",
    "print(classes)\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7079f42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-18.507832 -40.332214 -29.901289 -27.833338 -34.036144 -33.245827\n",
      "  -33.63047  -48.829025 -30.250298 -39.242725 -36.56614  -46.36462\n",
      "  -37.729874 -31.031254 -23.946598 -29.331432 -26.948265 -34.845177\n",
      "  -29.84082  -32.404846 -33.244797 -22.313288 -34.782978 -19.928398\n",
      "  -23.757126 -27.861801 -34.82058  -31.518398 -44.084396 -30.614248\n",
      "  -33.975758 -39.423607 -40.932915 -38.541206 -39.67091  -27.326248\n",
      "  -37.473133 -32.906624 -40.19321  -25.875923 -40.66318  -40.534676\n",
      "  -36.15323  -39.913033 -37.405014 -34.13552  -43.347378 -39.797394\n",
      "  -42.164772 -29.535353 -36.049953 -30.733437 -39.15055  -37.24141\n",
      "  -38.212837 -32.006767 -31.875906 -36.86821  -36.645813 -29.377214\n",
      "  -33.039642 -26.502087 -26.268223 -31.962337 -28.613312 -37.545956\n",
      "  -29.341995 -32.33619  -38.466007 -35.918888 -37.352306 -26.549026\n",
      "  -30.242155 -16.615776 -37.71755  -35.303677 -34.35027  -32.270493\n",
      "  -33.92288  -34.018555 -42.512962 -36.09566  -36.33154  -39.230522\n",
      "  -32.860424 -33.70783  -29.430367 -34.148537 -36.02753  -36.255936\n",
      "  -36.415276 -23.285141 -31.647865 -20.477066 -38.526047 -38.48978\n",
      "  -36.503517 -36.93421  -25.068516 -32.635025 -32.502403 -42.704975\n",
      "  -30.954212 -40.571453 -24.641987 -32.761147 -19.283848 -38.682693\n",
      "  -34.053284 -39.64705  -28.787218 -33.944355 -26.299465 -28.804539\n",
      "  -31.015324 -35.31084  -32.992798 -32.460953 -30.64904  -33.195965\n",
      "  -44.03635  -39.4209   -33.475822 -28.959417 -39.67034  -35.567932\n",
      "  -35.580902 -36.492233 -36.715622 -35.24306  -22.982145 -37.321033\n",
      "  -33.125008 -32.165543 -35.96187  -31.21615  -34.850674 -26.526691\n",
      "  -35.130886 -32.52199  -38.36454  -34.72189  -25.26613  -40.55088\n",
      "  -28.617577 -31.100103 -37.6557   -24.163116]]\n"
     ]
    }
   ],
   "source": [
    "# with cosine simoilarity\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transformation for input images (should match the one used during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load the trained model\n",
    "model_path = 'cattle_aadhar_copy.model'\n",
    "model = ConvNet(num_classes=len(classes))  # Update with the correct number of classes\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load and preprocess the input image\n",
    "input_image_path = r\"E:\\g42\\051\\IMG_0368.JPG\"  # Update with the path to your image\n",
    "input_image = Image.open(input_image_path)\n",
    "input_tensor = transform(input_image)\n",
    "input_tensor = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# print(output)\n",
    "\n",
    "input_image = output.numpy()\n",
    "\n",
    "print(input_image)\n",
    "\n",
    "tensor_input = torch.tensor(input_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99ef9bcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9911370873451233\n",
      "5\n",
      "prediction class is  164_muzzle\n"
     ]
    }
   ],
   "source": [
    "# to acccess each image present in each cattle id\n",
    "\n",
    "b = False\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "parent_directory = r\"E:\\g42\\cattle_final_dataset\\train\"  # Replace with the path to your \"cattle\" directory\n",
    "\n",
    "sl=[]\n",
    "cl=[]\n",
    "\n",
    "# Iterate through each subdirectory (representing different cattle)\n",
    "for cattle_id in os.listdir(parent_directory):\n",
    "    cattle_directory = os.path.join(parent_directory, cattle_id)\n",
    "\n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(cattle_directory):\n",
    "#         print(f\"Accessing cattle ID: {cattle_id}\")\n",
    "\n",
    "        maxi=0\n",
    "\n",
    "        # Iterate through image files in the cattle's directory\n",
    "        for image_file in os.listdir(cattle_directory):\n",
    "            if image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                \n",
    "                image_path = os.path.join(cattle_directory, image_file)\n",
    "#               print(f\"Processing image: {image_path}\")\n",
    "                    \n",
    "                # to replace \\ by \\\\\n",
    "                    \n",
    "                image_path='r'+image_path[0:]\n",
    "                i = 0\n",
    "                # print(image_path)\n",
    "                while i < len(image_path):\n",
    "                # print(image_path[i])\n",
    "                    if image_path[i] == '\\\\':\n",
    "                        i = i + 1\n",
    "                        image_path = image_path[:i] + '\\\\' + image_path[i:]\n",
    "                    i = i + 1\n",
    "                image_path=image_path[1:]\n",
    "#               print(image_path)\n",
    "                    \n",
    "                # Load and preprocess the input image\n",
    "                input_image_path = image_path  # Update with the path to your image\n",
    "                input_image = Image.open(input_image_path)\n",
    "                input_tensor = transform(input_image)\n",
    "                input_tensor = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "                # Make predictions\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_tensor)\n",
    "                    \n",
    "                class_checker = output.numpy()\n",
    "                    \n",
    "                tensor_class = torch.tensor(class_checker)\n",
    "                    \n",
    "                similarity=F.cosine_similarity(tensor_input, tensor_class)\n",
    "                \n",
    "                if((similarity.item())>0.98 and (similarity.item())>maxi):\n",
    "                    b=True\n",
    "                    maxi=similarity.item()\n",
    "#                     print(maxi)\n",
    "#               print(class_checker)\n",
    "\n",
    "\n",
    "#               print(\"prediction class is \",cattle_id)\n",
    "        if(b): \n",
    "            b=False\n",
    "            cl.append(cattle_id)\n",
    "            sl.append(maxi)\n",
    "                    # Here, you can perform any processing you need on the image\n",
    "\n",
    "\n",
    "\n",
    "# Note: Replace the extensions in `endswith` with the image formats you want to process\n",
    "\n",
    "\n",
    "# print(cl)   # to cheeck silimar classes\n",
    "# print(sl)    # to cheeck silimarity of classes\n",
    "\n",
    "if(len(sl)!=0):\n",
    "    b=True\n",
    "else:\n",
    "    b=False\n",
    "\n",
    "maximum=0\n",
    "\n",
    "for i in sl:\n",
    "    if(i>maximum):\n",
    "        maximum=i\n",
    "        \n",
    "from IPython.display import Image\n",
    "\n",
    "if(not b):\n",
    "    print(\"cant find this registration\")\n",
    "else:\n",
    "    # print(sl)\n",
    "    print(maximum)\n",
    "    ind=sl.index(maximum)\n",
    "    print(ind)\n",
    "    print(\"prediction class is \",cl[ind])\n",
    "#     print_face_of_prediction=os.path.join(\"E:\\g42\\cattle_final_dataset_face\",cl[ind])\n",
    "\n",
    "#     folder_path =os.path.join(\"E:\\g42\\cattle_final_dataset_face\",cl[ind])  # Replace with the path to your folder\n",
    "\n",
    "# #     print(folder_path)\n",
    "\n",
    "\n",
    "#     # Check if the folder exists\n",
    "#     if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "#         # List image files in the folder\n",
    "#         image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'))]\n",
    "\n",
    "#         if not image_files:\n",
    "#             print(\"No image files found in the folder.\")\n",
    "#         else:\n",
    "#             # Display the first image (you can change the index to display a different image)\n",
    "#             first_image_path = os.path.join(folder_path, image_files[0])\n",
    "#             display(Image(filename=first_image_path,width=200))\n",
    "#     else:\n",
    "#         print(\"Folder not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43a1b83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-18.507832 -40.332214 -29.901289 -27.833338 -34.036144 -33.245827\n",
      "  -33.63047  -48.829025 -30.250298 -39.242725 -36.56614  -46.36462\n",
      "  -37.729874 -31.031254 -23.946598 -29.331432 -26.948265 -34.845177\n",
      "  -29.84082  -32.404846 -33.244797 -22.313288 -34.782978 -19.928398\n",
      "  -23.757126 -27.861801 -34.82058  -31.518398 -44.084396 -30.614248\n",
      "  -33.975758 -39.423607 -40.932915 -38.541206 -39.67091  -27.326248\n",
      "  -37.473133 -32.906624 -40.19321  -25.875923 -40.66318  -40.534676\n",
      "  -36.15323  -39.913033 -37.405014 -34.13552  -43.347378 -39.797394\n",
      "  -42.164772 -29.535353 -36.049953 -30.733437 -39.15055  -37.24141\n",
      "  -38.212837 -32.006767 -31.875906 -36.86821  -36.645813 -29.377214\n",
      "  -33.039642 -26.502087 -26.268223 -31.962337 -28.613312 -37.545956\n",
      "  -29.341995 -32.33619  -38.466007 -35.918888 -37.352306 -26.549026\n",
      "  -30.242155 -16.615776 -37.71755  -35.303677 -34.35027  -32.270493\n",
      "  -33.92288  -34.018555 -42.512962 -36.09566  -36.33154  -39.230522\n",
      "  -32.860424 -33.70783  -29.430367 -34.148537 -36.02753  -36.255936\n",
      "  -36.415276 -23.285141 -31.647865 -20.477066 -38.526047 -38.48978\n",
      "  -36.503517 -36.93421  -25.068516 -32.635025 -32.502403 -42.704975\n",
      "  -30.954212 -40.571453 -24.641987 -32.761147 -19.283848 -38.682693\n",
      "  -34.053284 -39.64705  -28.787218 -33.944355 -26.299465 -28.804539\n",
      "  -31.015324 -35.31084  -32.992798 -32.460953 -30.64904  -33.195965\n",
      "  -44.03635  -39.4209   -33.475822 -28.959417 -39.67034  -35.567932\n",
      "  -35.580902 -36.492233 -36.715622 -35.24306  -22.982145 -37.321033\n",
      "  -33.125008 -32.165543 -35.96187  -31.21615  -34.850674 -26.526691\n",
      "  -35.130886 -32.52199  -38.36454  -34.72189  -25.26613  -40.55088\n",
      "  -28.617577 -31.100103 -37.6557   -24.163116]]\n"
     ]
    }
   ],
   "source": [
    "# with corelation factor\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transformation for input images (should match the one used during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load the trained model\n",
    "model_path = 'cattle_aadhar_copy.model'\n",
    "model = ConvNet(num_classes=len(classes))  # Update with the correct number of classes\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load and preprocess the input image\n",
    "input_image_path = r\"E:\\g42\\051\\IMG_0368.JPG\"  # Update with the path to your image\n",
    "input_image = Image.open(input_image_path)\n",
    "input_tensor = transform(input_image)\n",
    "input_tensor = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# print(output)\n",
    "\n",
    "input_image = output.numpy()\n",
    "\n",
    "print(input_image)\n",
    "\n",
    "corelation_input = input_image.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3baad2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Coefficient: 0.8143159906393567\n",
      "[]\n",
      "[]\n",
      "cant find this registration\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# to acccess each image present in each cattle id\n",
    "\n",
    "b = False\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "parent_directory = r\"E:\\g42\\cattle_final_dataset\\train\"  # Replace with the path to your \"cattle\" directory\n",
    "\n",
    "sl=[]\n",
    "cl=[]\n",
    "\n",
    "# Iterate through each subdirectory (representing different cattle)\n",
    "for cattle_id in os.listdir(parent_directory):\n",
    "    cattle_directory = os.path.join(parent_directory, cattle_id)\n",
    "\n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(cattle_directory):\n",
    "#         print(f\"Accessing cattle ID: {cattle_id}\")\n",
    "\n",
    "        maxi=0\n",
    "\n",
    "        # Iterate through image files in the cattle's directory\n",
    "        for image_file in os.listdir(cattle_directory):\n",
    "            if image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                \n",
    "                image_path = os.path.join(cattle_directory, image_file)\n",
    "#               print(f\"Processing image: {image_path}\")\n",
    "                    \n",
    "                # to replace \\ by \\\\\n",
    "                    \n",
    "                image_path='r'+image_path[0:]\n",
    "                i = 0\n",
    "                # print(image_path)\n",
    "                while i < len(image_path):\n",
    "                # print(image_path[i])\n",
    "                    if image_path[i] == '\\\\':\n",
    "                        i = i + 1\n",
    "                        image_path = image_path[:i] + '\\\\' + image_path[i:]\n",
    "                    i = i + 1\n",
    "                image_path=image_path[1:]\n",
    "#               print(image_path)\n",
    "                    \n",
    "                # Load and preprocess the input image\n",
    "                input_image_path = image_path  # Update with the path to your image\n",
    "                input_image = Image.open(input_image_path)\n",
    "                input_tensor = transform(input_image)\n",
    "                input_tensor = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "                # Make predictions\n",
    "                with torch.no_grad():\n",
    "                    output = model(input_tensor)\n",
    "                    \n",
    "                class_checker = output.numpy()\n",
    "                    \n",
    "                corelation_class = class_checker.flatten()\n",
    "                    \n",
    "                similarity=np.corrcoef(corelation_input, corelation_class)[0, 1]\n",
    "                \n",
    "                if((similarity.item())>0.98 and (similarity.item())>maxi):\n",
    "                    b=True\n",
    "                    maxi=similarity.item()\n",
    "#                     print(maxi)\n",
    "#               print(class_checker)\n",
    "\n",
    "\n",
    "#               print(\"prediction class is \",cattle_id)\n",
    "        if(b): \n",
    "            b=False\n",
    "            cl.append(cattle_id)\n",
    "            sl.append(maxi)\n",
    "                    # Here, you can perform any processing you need on the image\n",
    "\n",
    "\n",
    "\n",
    "# Note: Replace the extensions in `endswith` with the image formats you want to process\n",
    "\n",
    "\n",
    "print(cl)   # to cheeck silimar classes\n",
    "print(sl)    # to cheeck silimarity of classes\n",
    "\n",
    "if(len(sl)!=0):\n",
    "    b=True\n",
    "else:\n",
    "    b=False\n",
    "\n",
    "maximum=0\n",
    "\n",
    "for i in sl:\n",
    "    if(i>maximum):\n",
    "        maximum=i\n",
    "        \n",
    "from IPython.display import Image\n",
    "\n",
    "if(not b):\n",
    "    print(\"cant find this registration\")\n",
    "else:\n",
    "    # print(sl)\n",
    "    print(maximum)\n",
    "    ind=sl.index(maximum)\n",
    "    print(ind)\n",
    "    print(\"prediction class is \",cl[ind])\n",
    "#     print_face_of_prediction=os.path.join(\"E:\\g42\\cattle_final_dataset_face\",cl[ind])\n",
    "\n",
    "#     folder_path =os.path.join(\"E:\\g42\\cattle_final_dataset_face\",cl[ind])  # Replace with the path to your folder\n",
    "\n",
    "# #     print(folder_path)\n",
    "\n",
    "\n",
    "#     # Check if the folder exists\n",
    "#     if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "#         # List image files in the folder\n",
    "#         image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'))]\n",
    "\n",
    "#         if not image_files:\n",
    "#             print(\"No image files found in the folder.\")\n",
    "#         else:\n",
    "#             # Display the first image (you can change the index to display a different image)\n",
    "#             first_image_path = os.path.join(folder_path, image_files[0])\n",
    "#             display(Image(filename=first_image_path,width=200))\n",
    "#     else:\n",
    "#         print(\"Folder not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to display the head of the cattle after it predicts the class\n",
    "\n",
    "\n",
    "\n",
    "# from IPython.display import Image\n",
    "# import os\n",
    "\n",
    "# folder_path =os.path.join(\"E:\\g42\\cattle_final_dataset_face\",cl[ind])  # Replace with the path to your folder\n",
    "\n",
    "# print(folder_path)\n",
    "\n",
    "\n",
    "# # Check if the folder exists\n",
    "# if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "#     # List image files in the folder\n",
    "#     image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'))]\n",
    "\n",
    "#     if not image_files:\n",
    "#         print(\"No image files found in the folder.\")\n",
    "#     else:\n",
    "#         # Display the first image (you can change the index to display a different image)\n",
    "#         first_image_path = os.path.join(folder_path, image_files[0])\n",
    "#         display(Image(filename=first_image_path,width=200))\n",
    "# else:\n",
    "#     print(\"Folder not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0204b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c03fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052cd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209eba4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c99ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936bc09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17025c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b8bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75adf08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292532e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7868a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba185b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409bd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991695d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379236c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6523d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CNN Network\n",
    "\n",
    "\n",
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self,num_classes=6):\n",
    "#         super(ConvNet,self).__init__()\n",
    "        \n",
    "#         #Output size after convolution filter\n",
    "#         #((w-f+2P)/s) +1\n",
    "        \n",
    "#         #Input shape= (256,3,150,150)\n",
    "        \n",
    "#         self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "#         #Shape= (256,12,150,150)\n",
    "#         self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "#         #Shape= (256,12,150,150)\n",
    "#         self.relu1=nn.ReLU()\n",
    "#         #Shape= (256,12,150,150)\n",
    "        \n",
    "#         self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "#         #Reduce the image size be factor 2\n",
    "#         #Shape= (256,12,75,75)\n",
    "        \n",
    "        \n",
    "#         self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "#         #Shape= (256,20,75,75)\n",
    "#         self.relu2=nn.ReLU()\n",
    "#         #Shape= (256,20,75,75)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "#         #Shape= (256,32,75,75)\n",
    "#         self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "#         #Shape= (256,32,75,75)\n",
    "#         self.relu3=nn.ReLU()\n",
    "#         #Shape= (256,32,75,75)\n",
    "        \n",
    "        \n",
    "#         self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         #Feed forwad function\n",
    "        \n",
    "#     def forward(self,input):\n",
    "#         output=self.conv1(input)\n",
    "#         output=self.bn1(output)\n",
    "#         output=self.relu1(output)\n",
    "            \n",
    "#         output=self.pool(output)\n",
    "            \n",
    "#         output=self.conv2(output)\n",
    "#         output=self.relu2(output)\n",
    "            \n",
    "#         output=self.conv3(output)\n",
    "#         output=self.bn3(output)\n",
    "#         output=self.relu3(output)\n",
    "            \n",
    "            \n",
    "#             #Above output will be in matrix form, with shape (256,32,75,75)\n",
    "            \n",
    "#         output=output.view(-1,32*75*75)\n",
    "            \n",
    "            \n",
    "#         output=self.fc(output)\n",
    "            \n",
    "#         return output\n",
    "       \n",
    "\n",
    "# #Transforms\n",
    "# transformer=transforms.Compose([\n",
    "#     transforms.Resize((150,150)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "#     transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "#                         [0.5,0.5,0.5])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #categories\n",
    "# dataset_path = r\"E:\\g42\\cattle_final_dataset\"\n",
    "# root=pathlib.Path(os.path.join(dataset_path,'train'))\n",
    "# classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86166264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classes)\n",
    "\n",
    "# print(\"\")\n",
    "\n",
    "# print(\"len\",len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Create an instance of your model\n",
    "# model = ConvNet(num_classes=len(classes))  # Replace with your actual model class and configuration\n",
    "\n",
    "# # Load the saved model weights\n",
    "# model.load_state_dict(torch.load('cattle_aadhar_copy.model'))\n",
    "\n",
    "# # Set the model in evaluation mode (important if you're using dropout or batch normalization)\n",
    "# model.eval()\n",
    "\n",
    "# # import torch\n",
    "\n",
    "# # # Create an instance of your model\n",
    "# # model = ConvNet(num_classes=len(classes))  # Replace with your actual model class and configuration\n",
    "\n",
    "# # # Load the saved model weights\n",
    "# # model.load_state_dict(torch.load('cattle_aadhar_100epochs.model'))\n",
    "\n",
    "# # # Set the model in evaluation mode (important if you're using dropout or batch normalization)\n",
    "# # model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import os\n",
    "\n",
    "\n",
    "# # Load your existing ConvNet model (assuming you have trained it)\n",
    "# model = ConvNet(num_classes=classes)  # Instantiate your model or load the pretrained weights\n",
    "\n",
    "# # Freeze feature extraction layers (if they are named 'features')\n",
    "# for param in model.features.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Define a new classification layer for class 151\n",
    "# num_classes = len(classes)+1  # Including the new class\n",
    "# in_features = model.fc.in_features  # Get the number of in_features for the classification layer\n",
    "# model.fc = nn.Linear(in_features, num_classes)  # Replace the classification layer\n",
    "\n",
    "# # Define a custom dataset class for class 151\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, data_dir, transform=None):\n",
    "#         self.data = torchvision.datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx]\n",
    "\n",
    "# # Create a DataLoader for the new class (class 151)\n",
    "# new_class_dataset = CustomDataset(data_dir=dataset_path, transform=transforms.Compose([\n",
    "#     transforms.Resize((150, 150)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "# ]))\n",
    "\n",
    "# new_class_loader = DataLoader(new_class_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Define loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# # Train the model on the new class\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for images, labels in new_class_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# # Save the updated model\n",
    "# torch.save(model.state_dict(), 'incremental_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d2c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353cc50f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Transforms\n",
    "# transformer=transforms.Compose([\n",
    "#     transforms.Resize((150,150)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.CenterCrop(150),\n",
    "#     transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "#     transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "#                         [0.5,0.5,0.5])\n",
    "# ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
